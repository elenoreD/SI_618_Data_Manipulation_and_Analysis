{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SI 618: Data Manipulation and Analysis\n",
    "## 07 - Natural Language Processing\n",
    "### Dr. Chris Teplovs, School of Information, University of Michigan\n",
    "<small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a> This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline for today\n",
    "\n",
    "- regular expressions redux\n",
    "- ```nltk```\n",
    "    - Cleaning the data\n",
    "    - Extracting linguistic features\n",
    "- ```Word2Vec```\n",
    "    - Vector representation of words\n",
    "    - Word similarities\n",
    "    - Vector algebra for semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# Why learn NLP?\n",
    "- Natural language = human language\n",
    "- We use language to learn about the world\n",
    "- How machines understand human langauge?\n",
    "- How can we quantify the meaning of language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications?\n",
    "- Probabily any service that uses text as information\n",
    "- Search engine, SNS\n",
    "    - What's the document about?\n",
    "    - How do you determine the similarity?\n",
    "- Virtual assistants: Alexa, Google Assistant, Cortana, etc. \n",
    "    - Understand the semantic information from your speech from parsed text\n",
    "- Biology, genetics\n",
    "    - Genetic information / DNA sequence as text\n",
    "    - Draw networks of proteins/molecules from vast amount of scientific papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** we have included a [cheat sheet for regular expressions](python-regular-expressions-cheat-sheet.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are simply a way to find sequences of characters within strings.  Let's use, as our text, \"Bereft\", a poem by Robert Frost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bereft = \"\"\"Where had I heard this wind before\n",
    "Change like this to a deeper roar?\n",
    "What would it take my standing there for,\n",
    "Holding open a restive door,\n",
    "Looking down hill to a frothy shore?\n",
    "Summer was past and the day was past.\n",
    "Sombre clouds in the west were massed.\n",
    "Out on the porch's sagging floor,\n",
    "Leaves got up in a coil and hissed,\n",
    "Blindly striking at my knee and missed.\n",
    "Something sinister in the tone\n",
    "Told me my secret my be known:\n",
    "Word I was in the house alone\n",
    "Somehow must have gotten abroad,\n",
    "Word I was in my life alone,\n",
    "Word I had no one left but God.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our lives simpler (we'll discuss why this is the case in class), we're going to strip the newlines from the passage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bereft = bereft.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Where had I heard this wind before Change like this to a deeper roar? What would it take my standing there for, Holding open a restive door, Looking down hill to a frothy shore? Summer was past and the day was past. Sombre clouds in the west were massed. Out on the porch's sagging floor, Leaves got up in a coil and hissed, Blindly striking at my knee and missed. Something sinister in the tone Told me my secret my be known: Word I was in the house alone Somehow must have gotten abroad, Word I was in my life alone, Word I had no one left but God.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bereft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to find all the occurrences of the word \"alone\".   We could use  plain old string functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bereft.find('alone') # 451 is the postion of 'alone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yup, found it\n"
     ]
    }
   ],
   "source": [
    "if 'alone' in bereft:\n",
    "    print('Yup, found it')\n",
    "else:\n",
    "    print('Hot there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bereft.count('alone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now try it yourself:  \n",
    "### <font color=\"magenta\">Q1: How many times does the word ```was``` appear in the poem?</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bereft.count('was')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good.  Now let's make things a bit more interesting.  How many words are there that contain the letters ```one``` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bereft.count('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'one', 'one', 'one']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('one',bereft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we wanted to know the words that contained ```one``` instead of just the count?  Enter regular expressions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[a-z]+one',bereft) # + means there must be somthing\n",
    "re.findall('[a-z]*one',bereft) # * means something or nothing there\n",
    "for ele in re.findall('[a-z]*one',bereft):\n",
    "    a = bereft.count(ele)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful online resources:\n",
    "\n",
    "* www.debuggex.com\n",
    "* www.regexr.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(391, 395), match='tone'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('[a-z]*one',bereft) # return a match object, similar to group_by function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search('[a-z]*one', bereft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it!\n"
     ]
    }
   ],
   "source": [
    "if match:\n",
    "    print(\"Found it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it!\n",
      "tone\n"
     ]
    }
   ],
   "source": [
    "if match:\n",
    "    print(\"Found it!\")\n",
    "    print(match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Groups\n",
    "\n",
    "In the above example, we used ```match.group(0)``` to extract the entire match.\n",
    "\n",
    "Match groups also allow you to extract only certain parts of the match.  In the previous example, say we wanted to know which letters preceded the letters 'one'.  We could use match groups, specified by paretheses, to extract only certain parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search('([a-z]*)one', bereft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found it!\n",
      "tone\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "if match:\n",
    "    print(\"Found it!\")\n",
    "    print(match.group(0))\n",
    "    print(match.group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we extract all the letters that precede *one*?  Use ```re.finditer()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.finditer('([a-z]*)one', bereft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tone t\n",
      "alone al\n",
      "alone al\n",
      "one \n"
     ]
    }
   ],
   "source": [
    "for match in matches:\n",
    "    print(match.group(0),match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alone al a l\n",
      "alone al a l\n"
     ]
    }
   ],
   "source": [
    "matches = re.finditer('(([a-z])([a-z]))one', bereft)\n",
    "for match in matches:\n",
    "    print(match.group(0),match.group(1),match.group(2),match.group(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where had I heard this wind before Change like this to a deeper roar? What would it take my standing there for',\n",
       " ' Holding open a restive door',\n",
       " \" Looking down hill to a frothy shore? Summer was past and the day was past. Sombre clouds in the west were massed. Out on the porch's sagging floor\",\n",
       " ' Leaves got up in a coil and hissed',\n",
       " ' Blindly striking at my knee and missed. Something sinister in the tone Told me my secret my be known: Word I was in the house alone Somehow must have gotten abroad',\n",
       " ' Word I was in my life alone',\n",
       " ' Word I had no one left but God.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(',',bereft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2: Experiment with various regular expressions such as \\W, \\w, \\s, \\S to see how the poem can be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where',\n",
       " 'had',\n",
       " 'I',\n",
       " 'heard',\n",
       " 'this',\n",
       " 'wind',\n",
       " 'before',\n",
       " 'Change',\n",
       " 'like',\n",
       " 'this',\n",
       " 'to',\n",
       " 'a',\n",
       " 'deeper',\n",
       " 'roar',\n",
       " '',\n",
       " 'What',\n",
       " 'would',\n",
       " 'it',\n",
       " 'take',\n",
       " 'my',\n",
       " 'standing',\n",
       " 'there',\n",
       " 'for',\n",
       " '',\n",
       " 'Holding',\n",
       " 'open',\n",
       " 'a',\n",
       " 'restive',\n",
       " 'door',\n",
       " '',\n",
       " 'Looking',\n",
       " 'down',\n",
       " 'hill',\n",
       " 'to',\n",
       " 'a',\n",
       " 'frothy',\n",
       " 'shore',\n",
       " '',\n",
       " 'Summer',\n",
       " 'was',\n",
       " 'past',\n",
       " 'and',\n",
       " 'the',\n",
       " 'day',\n",
       " 'was',\n",
       " 'past',\n",
       " '',\n",
       " 'Sombre',\n",
       " 'clouds',\n",
       " 'in',\n",
       " 'the',\n",
       " 'west',\n",
       " 'were',\n",
       " 'massed',\n",
       " '',\n",
       " 'Out',\n",
       " 'on',\n",
       " 'the',\n",
       " 'porch',\n",
       " 's',\n",
       " 'sagging',\n",
       " 'floor',\n",
       " '',\n",
       " 'Leaves',\n",
       " 'got',\n",
       " 'up',\n",
       " 'in',\n",
       " 'a',\n",
       " 'coil',\n",
       " 'and',\n",
       " 'hissed',\n",
       " '',\n",
       " 'Blindly',\n",
       " 'striking',\n",
       " 'at',\n",
       " 'my',\n",
       " 'knee',\n",
       " 'and',\n",
       " 'missed',\n",
       " '',\n",
       " 'Something',\n",
       " 'sinister',\n",
       " 'in',\n",
       " 'the',\n",
       " 'tone',\n",
       " 'Told',\n",
       " 'me',\n",
       " 'my',\n",
       " 'secret',\n",
       " 'my',\n",
       " 'be',\n",
       " 'known',\n",
       " '',\n",
       " 'Word',\n",
       " 'I',\n",
       " 'was',\n",
       " 'in',\n",
       " 'the',\n",
       " 'house',\n",
       " 'alone',\n",
       " 'Somehow',\n",
       " 'must',\n",
       " 'have',\n",
       " 'gotten',\n",
       " 'abroad',\n",
       " '',\n",
       " 'Word',\n",
       " 'I',\n",
       " 'was',\n",
       " 'in',\n",
       " 'my',\n",
       " 'life',\n",
       " 'alone',\n",
       " '',\n",
       " 'Word',\n",
       " 'I',\n",
       " 'had',\n",
       " 'no',\n",
       " 'one',\n",
       " 'left',\n",
       " 'but',\n",
       " 'God',\n",
       " '']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\W',bereft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '? ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ', ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ', ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '? ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '. ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '. ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " \"'\",\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ', ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ', ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '. ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ': ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ', ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ', ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\w',bereft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\S',bereft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where',\n",
       " 'had',\n",
       " 'I',\n",
       " 'heard',\n",
       " 'this',\n",
       " 'wind',\n",
       " 'before',\n",
       " 'Change',\n",
       " 'like',\n",
       " 'this',\n",
       " 'to',\n",
       " 'a',\n",
       " 'deeper',\n",
       " 'roar?',\n",
       " 'What',\n",
       " 'would',\n",
       " 'it',\n",
       " 'take',\n",
       " 'my',\n",
       " 'standing',\n",
       " 'there',\n",
       " 'for,',\n",
       " 'Holding',\n",
       " 'open',\n",
       " 'a',\n",
       " 'restive',\n",
       " 'door,',\n",
       " 'Looking',\n",
       " 'down',\n",
       " 'hill',\n",
       " 'to',\n",
       " 'a',\n",
       " 'frothy',\n",
       " 'shore?',\n",
       " 'Summer',\n",
       " 'was',\n",
       " 'past',\n",
       " 'and',\n",
       " 'the',\n",
       " 'day',\n",
       " 'was',\n",
       " 'past.',\n",
       " 'Sombre',\n",
       " 'clouds',\n",
       " 'in',\n",
       " 'the',\n",
       " 'west',\n",
       " 'were',\n",
       " 'massed.',\n",
       " 'Out',\n",
       " 'on',\n",
       " 'the',\n",
       " \"porch's\",\n",
       " 'sagging',\n",
       " 'floor,',\n",
       " 'Leaves',\n",
       " 'got',\n",
       " 'up',\n",
       " 'in',\n",
       " 'a',\n",
       " 'coil',\n",
       " 'and',\n",
       " 'hissed,',\n",
       " 'Blindly',\n",
       " 'striking',\n",
       " 'at',\n",
       " 'my',\n",
       " 'knee',\n",
       " 'and',\n",
       " 'missed.',\n",
       " 'Something',\n",
       " 'sinister',\n",
       " 'in',\n",
       " 'the',\n",
       " 'tone',\n",
       " 'Told',\n",
       " 'me',\n",
       " 'my',\n",
       " 'secret',\n",
       " 'my',\n",
       " 'be',\n",
       " 'known:',\n",
       " 'Word',\n",
       " 'I',\n",
       " 'was',\n",
       " 'in',\n",
       " 'the',\n",
       " 'house',\n",
       " 'alone',\n",
       " 'Somehow',\n",
       " 'must',\n",
       " 'have',\n",
       " 'gotten',\n",
       " 'abroad,',\n",
       " 'Word',\n",
       " 'I',\n",
       " 'was',\n",
       " 'in',\n",
       " 'my',\n",
       " 'life',\n",
       " 'alone,',\n",
       " 'Word',\n",
       " 'I',\n",
       " 'had',\n",
       " 'no',\n",
       " 'one',\n",
       " 'left',\n",
       " 'but',\n",
       " 'God.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s',bereft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load some data for use later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link='https://raw.githubusercontent.com/vineetdhanawat/twitter-sentiment-analysis/master/datasets/Sentiment%20Analysis%20Dataset.csv'\n",
    "# sentiment_df = pd.read_csv(link,encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Install the nltk and gensim libraries now.  Windows users may need to implement some work-arounds to get spaCy to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -y nltk gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## While that's running, how about a few rounds of regex golf?\n",
    "\n",
    "### <font color=\"magenta\">Q3: See how well you can do in your groups (divide up into 9 groups): https://alf.nu/RegexGolf</a></font>\n",
    "Record your final score below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "* often implemented as a pipeline with one or more of the following steps:\n",
    " * normalization\n",
    " * tokenization\n",
    " * stemming\n",
    " * lemmatization\n",
    " * part-of-speech tagging\n",
    " * named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk\n",
    "The Natural Language ToolKit (nltk) is the most common NLP package for python.\n",
    "\n",
    "https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/aaa/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You should only ever need to do this once\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"This is a very simple sentence, which is used for teaching natural language processing, that Chris didn't write on Saturday.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'simple',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'used',\n",
       " 'for',\n",
       " 'teaching',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " ',',\n",
       " 'that',\n",
       " 'Chris',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'write',\n",
       " 'on',\n",
       " 'Saturday',\n",
       " '.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "\n",
    "Words that has small contribution to the meaning of phrases but appear frequently. It is advised to remove them in most tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# load list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed:  simple sentence , used teaching natural language processing , Chris n't write Saturday .\n"
     ]
    }
   ],
   "source": [
    "S = set(stopwords.words('english'))\n",
    "tokens_stop_removed = []\n",
    "for token in tokens:\n",
    "    if not token.lower() in S:\n",
    "        tokens_stop_removed.append(token)\n",
    "sent_stop_removed = ' '.join(tokens_stop_removed)\n",
    "print(\"Stopwords removed: \",sent_stop_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 2),\n",
       " ('simple', 1),\n",
       " ('sentence', 1),\n",
       " ('used', 1),\n",
       " ('teaching', 1),\n",
       " ('natural', 1),\n",
       " ('language', 1),\n",
       " ('processing', 1),\n",
       " ('Chris', 1),\n",
       " (\"n't\", 1)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(tokens_stop_removed).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech tagging\n",
    "\n",
    "Sometimes it's useful to know what part of speech (e.g. noun, verb, adjective, adverb, etc.) a word is.  nltk supports part-of-speech tagging.  Here's a list of what the POS codes mean (from https://www.guru99.com/pos-tagging-chunking-nltk.html)\n",
    "\n",
    "```\n",
    "\n",
    "Abbreviation\tMeaning\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective (large)\n",
    "JJR\tadjective, comparative (larger)\n",
    "JJS\tadjective, superlative (largest)\n",
    "LS\tlist market\n",
    "MD\tmodal (could, will)\n",
    "NN\tnoun, singular (cat, tree)\n",
    "NNS\tnoun plural (desks)\n",
    "NNP\tproper noun, singular (sarah)\n",
    "NNPS\tproper noun, plural (indians or americans)\n",
    "PDT\tpredeterminer (all, both, half)\n",
    "POS\tpossessive ending (parent\\ 's)\n",
    "PRP\tpersonal pronoun (hers, herself, him,himself)\n",
    "PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "RB\tadverb (occasionally, swiftly)\n",
    "RBR\tadverb, comparative (greater)\n",
    "RBS\tadverb, superlative (biggest)\n",
    "RP\tparticle (about)\n",
    "TO\tinfinite marker (to)\n",
    "UH\tinterjection (goodbye)\n",
    "VB\tverb (ask)\n",
    "VBG\tverb gerund (judging)\n",
    "VBD\tverb past tense (pleaded)\n",
    "VBN\tverb past participle (reunified)\n",
    "VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "WDT\twh-determiner (that, what)\n",
    "WP\twh- pronoun (who)\n",
    "WRB\twh- adverb (how)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('simple', 'JJ'),\n",
       " ('sentence', 'NN'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('used', 'VBN'),\n",
       " ('for', 'IN'),\n",
       " ('teaching', 'VBG'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " (',', ','),\n",
       " ('that', 'IN'),\n",
       " ('Chris', 'NNP'),\n",
       " ('did', 'VBD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('write', 'VB'),\n",
       " ('on', 'IN'),\n",
       " ('Saturday', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Remove suffix of word to generate a \"base\" word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "ran\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"run\", \"running\", \"ran\", \"runs\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is the process of obtaining the root or base form of a word.  It is a dictionary-based technique that does better with the more \"hints\" you give it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This (DT) => This\n",
      "is (VBZ) => be\n",
      "one (CD) => one\n",
      "of (IN) => of\n",
      "a (DT) => a\n",
      "series (NN) => series\n",
      "of (IN) => of\n",
      "notebooks (NNS) => notebook\n",
      "that (WDT) => that\n",
      "is (VBZ) => be\n",
      "designed (VBN) => design\n",
      "to (TO) => to\n",
      "help (VB) => help\n",
      "you (PRP) => you\n",
      "learn (VB) => learn\n",
      "about (IN) => about\n",
      "natural (JJ) => natural\n",
      "language (NN) => language\n",
      "processing (NN) => processing\n",
      ". (.) => .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "text = \"This is one of a series of notebooks that is designed to help you learn about natural language processing.\"\n",
    "tokens = word_tokenize(text)\n",
    "lemma_function = WordNetLemmatizer()\n",
    "for token, tag in pos_tag(tokens):\n",
    "    lemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n",
    "    print(\"{} ({}) => {}\".format( token, tag, lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  This/DT\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  very/RB\n",
      "  simple/JJ\n",
      "  sentence/NN\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  used/VBN\n",
      "  for/IN\n",
      "  teaching/VBG\n",
      "  natural/JJ\n",
      "  language/NN\n",
      "  processing/NN\n",
      "  ,/,\n",
      "  that/IN\n",
      "  (PERSON Chris/NNP)\n",
      "  did/VBD\n",
      "  n't/RB\n",
      "  write/VB\n",
      "  on/IN\n",
      "  Saturday/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                            S                                                                                                                \n",
      "    ________________________________________________________________________________________|___________________________________________________________________________________________________________      \n",
      "   |      |     |      |        |          |       |      |       |       |       |         |           |           |            |        |     |       |      |       |       |        |        |    PERSON \n",
      "   |      |     |      |        |          |       |      |       |       |       |         |           |           |            |        |     |       |      |       |       |        |        |      |     \n",
      "This/DT is/VBZ a/DT very/RB simple/JJ sentence/NN ,/, which/WDT is/VBZ used/VBN for/IN teaching/VBG natural/JJ language/NN processing/NN ,/, that/IN did/VBD n't/RB write/VB on/IN Saturday/NNP ./. Chris/NNP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entities.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tree.Tree"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q4: Show the top 10 most common words, excluding stopwords, from the following text (from Robert Frost's \"The Road Not Taken\").  Store the word tokens in a list called ```words```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_roads = \"\"\"Two roads diverged in a yellow wood,\n",
    "And sorry I could not travel both\n",
    "And be one traveller, long I stood\n",
    "And looked down one as far as I could\n",
    "To where it bent in the undergrowth;\n",
    "\n",
    "Then took the other, as just as fair,\n",
    "And having perhaps the better claim,\n",
    "Because it was grassy and wanted wear;\n",
    "Though as for that the passing there\n",
    "Had worn them really about the same,\n",
    "\n",
    "And both that morning equally lay\n",
    "In leaves no step had trodden black.\n",
    "Oh, I kept the first for another day!\n",
    "Yet knowing how way leads on to way,\n",
    "I doubted if I should ever come back.\n",
    "\n",
    "I shall be telling this with a sigh\n",
    "Somewhere ages and ages hence:\n",
    "Two roads diverged in a wood, and I—-\n",
    "I took the one less travelled by,\n",
    "And that has made all the difference.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two',\n",
       " 'roads',\n",
       " 'diverged',\n",
       " 'yellow',\n",
       " 'wood',\n",
       " ',',\n",
       " 'sorry',\n",
       " 'could',\n",
       " 'travel',\n",
       " 'one',\n",
       " 'traveller',\n",
       " ',',\n",
       " 'long',\n",
       " 'stood',\n",
       " 'looked',\n",
       " 'one',\n",
       " 'far',\n",
       " 'could',\n",
       " 'bent',\n",
       " 'undergrowth',\n",
       " ';',\n",
       " 'took',\n",
       " ',',\n",
       " 'fair',\n",
       " ',',\n",
       " 'perhaps',\n",
       " 'better',\n",
       " 'claim',\n",
       " ',',\n",
       " 'grassy',\n",
       " 'wanted',\n",
       " 'wear',\n",
       " ';',\n",
       " 'though',\n",
       " 'passing',\n",
       " 'worn',\n",
       " 'really',\n",
       " ',',\n",
       " 'morning',\n",
       " 'equally',\n",
       " 'lay',\n",
       " 'leaves',\n",
       " 'step',\n",
       " 'trodden',\n",
       " 'black',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'kept',\n",
       " 'first',\n",
       " 'another',\n",
       " 'day',\n",
       " '!',\n",
       " 'yet',\n",
       " 'knowing',\n",
       " 'way',\n",
       " 'leads',\n",
       " 'way',\n",
       " ',',\n",
       " 'doubted',\n",
       " 'ever',\n",
       " 'come',\n",
       " 'back',\n",
       " '.',\n",
       " 'shall',\n",
       " 'telling',\n",
       " 'sigh',\n",
       " 'somewhere',\n",
       " 'ages',\n",
       " 'ages',\n",
       " 'hence',\n",
       " ':',\n",
       " 'two',\n",
       " 'roads',\n",
       " 'diverged',\n",
       " 'wood',\n",
       " ',',\n",
       " 'i—-',\n",
       " 'took',\n",
       " 'one',\n",
       " 'less',\n",
       " 'travelled',\n",
       " ',',\n",
       " 'made',\n",
       " 'difference',\n",
       " '.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words= nltk.word_tokenize(two_roads)\n",
    "non_stop_words = []\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    if not word in S:\n",
    "        non_stop_words.append(word)\n",
    "words = non_stop_words\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to plot the frequency of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3240x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(45,10))\n",
    "sns.countplot(words, order=pd.Series(words).value_counts().index[:10])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(double click the plot to enlarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q5. Based on the word frequency results, what was the paragraph about? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "',' is the most frequent words in the poem. From the top 10 most common words, we can know the poem is about wood, roads, ages and diverged ways maybe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So far, we seen how we can extract some interesting syntactic characteristics from text from using ```nltk```\n",
    "- It extracted the characteristics, but did not indicate what it means\n",
    "- Can machines understand semantic relationship between words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Distributional semantics\n",
    "    - Representing semantic information of words in a geometric semantic space\n",
    "        - Different relationship between words: explained by geometric relationship between words \n",
    "        - e.g., Related words are located closer to each other; \n",
    "    - And it's often called as *word embedding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word2Vec\n",
    "- Developed by [Mikolov et al., 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- Represent the meaning of the words as a vector\n",
    "    - Vector: numeric array\n",
    "    - Output of a neural network model that predicts the next word\n",
    "- Surprisingly, many different semantic informations can be represented from word vectors of ```Word2Vec```\n",
    "- (More explanation in here: https://www.tensorflow.org/tutorials/representation/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/softmax-nplm.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://www.tensorflow.org/images/linear-relationships.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's try with some example: words in a semantic space\n",
    "$\\rightarrow$ https://projector.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q6. Record any interesting findings from TensorFlow Projector page</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very cool to see how words are related to each other at different levels. It's up to the model or the developer whoever's logic, how these relations be defined. Like for example, the word dog is related with the word cows more than the word dogs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK. Let's try some more details on our local machines!\n",
    "- We have included the [pretrained model](https://drive.google.com/open?id=10GXpuviDJVa-k8ZmiYX3BVABNDRaA6tg) as part of today's zip file, but there's the link for it if you ever need to download it again\n",
    "- We are using [gensim](https://radimrehurek.com/gensim/) package this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/eyaler/word2vec-slim\n",
    "w2v_mod = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300-SLIM.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating similarity between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: What's similarity between *school* and *student*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the word vector for *school* looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.70471478e-02,  1.14410305e-02,  1.49575379e-02,  8.87546614e-02,\n",
       "        3.96226160e-02, -2.67452672e-02,  6.33961856e-02, -1.90188568e-02,\n",
       "       -1.89445645e-03, -3.68490331e-02,  1.01037674e-01,  1.85235739e-02,\n",
       "        2.69433800e-02, -4.00188453e-02, -4.29905392e-02,  4.31886539e-02,\n",
       "       -8.12263638e-02,  5.72051527e-03,  5.54716662e-02, -3.56603563e-02,\n",
       "        8.32074955e-02,  6.93395808e-02,  4.72994987e-03,  6.97358102e-02,\n",
       "        1.96874887e-03, -1.41848966e-01,  9.22464067e-04,  7.48867467e-02,\n",
       "        4.85377051e-02, -1.02028241e-02,  4.14056368e-02, -4.33867648e-02,\n",
       "        1.62452739e-02,  3.04598873e-03, -6.61697686e-02, -6.06226064e-02,\n",
       "        9.27169248e-02, -2.04056483e-02,  1.88207440e-02,  5.07169515e-02,\n",
       "        5.29952534e-03,  5.19056283e-02,  4.47735563e-02, -2.05047056e-02,\n",
       "        1.39669729e-02,  5.86414747e-02,  6.97358102e-02, -1.12924464e-02,\n",
       "       -4.49716710e-02,  9.31131542e-02, -4.75471430e-02, -4.95282710e-02,\n",
       "       -1.44251098e-03, -4.61603515e-02,  8.59810784e-02, -8.47924054e-02,\n",
       "       -4.23962027e-02,  1.78301781e-02, -5.00235567e-03, -6.45848662e-02,\n",
       "       -3.58584709e-02, -1.62452739e-02,  4.31886539e-02, -2.06037611e-02,\n",
       "        3.14999819e-02,  5.94339259e-02, -7.01320320e-02,  8.39999467e-02,\n",
       "        8.96461774e-03,  1.64433867e-02, -7.44905248e-02,  4.91320454e-02,\n",
       "        3.52641307e-02,  1.49575379e-02, -4.51697856e-02,  1.44226328e-01,\n",
       "        1.16490498e-01,  1.29392615e-03,  7.01320320e-02,  6.41886443e-02,\n",
       "        2.02075355e-02, -6.77546784e-02, -6.10188320e-02,  8.43961760e-02,\n",
       "        2.23867781e-02,  1.60471611e-02,  1.50565952e-02,  9.62829590e-02,\n",
       "        6.73584491e-02, -1.14905592e-02, -7.37971254e-03, -7.68678784e-02,\n",
       "       -5.54716634e-03, -6.22075088e-02, -7.23112794e-03,  3.82358246e-02,\n",
       "        6.45848662e-02,  2.57547013e-02, -2.53584757e-02,  2.91226245e-02,\n",
       "       -1.94150824e-02, -5.42334560e-03, -5.62641174e-02,  2.48631928e-02,\n",
       "       -8.83584395e-02, -3.38773392e-02,  5.24999667e-03,  2.25848928e-02,\n",
       "       -8.81603267e-03, -8.43961760e-02, -9.35093760e-02,  6.89433515e-02,\n",
       "       -1.36698028e-02, -1.83254611e-02,  6.69622272e-02, -1.02152058e-04,\n",
       "        2.71414928e-02, -2.02075355e-02, -5.74527942e-02, -6.57735467e-02,\n",
       "       -4.55660112e-02, -6.16627000e-03,  1.00542391e-02, -8.81603267e-03,\n",
       "       -2.41697971e-02, -1.83254611e-02, -3.34811136e-02, -5.50754406e-02,\n",
       "        1.24439783e-03,  3.32829989e-02,  3.68490331e-02, -1.44226328e-01,\n",
       "       -7.88490102e-02, -4.14056368e-02, -5.38867600e-02, -1.21344263e-02,\n",
       "        1.10448049e-02, -4.23215215e-06, -1.12924464e-02,  2.85282843e-02,\n",
       "       -1.44226328e-01, -1.44226328e-01,  5.26980832e-02,  1.85731018e-03,\n",
       "       -1.05990507e-02,  8.71697590e-02, -3.92263904e-02, -3.86320539e-02,\n",
       "       -9.01414547e-03,  1.92695938e-04,  1.64037645e-01, -7.21131638e-02,\n",
       "       -3.66509221e-02,  6.06226064e-02, -8.36037248e-02, -5.23018539e-02,\n",
       "        1.03811257e-01, -9.70754102e-02,  4.35848795e-02, -2.95188501e-02,\n",
       "       -9.23207030e-02,  4.61603515e-02,  8.28112736e-02,  7.92452320e-02,\n",
       "       -3.14999819e-02,  4.68042167e-03, -1.24811241e-02, -8.51886272e-02,\n",
       "        2.48631928e-02,  2.13962141e-02, -8.83584395e-02,  1.59481037e-02,\n",
       "        6.43867534e-03, -1.06981069e-01,  6.83490140e-03,  2.65471544e-02,\n",
       "        6.98348647e-03,  4.97263856e-02,  4.23466740e-03,  1.00245222e-01,\n",
       "       -1.28773507e-03,  2.35754568e-02,  6.93395827e-03,  1.98113080e-02,\n",
       "        3.44716758e-02,  5.20046847e-03, -3.48679051e-02, -2.55565885e-02,\n",
       "        9.15282443e-02,  3.74433734e-02,  2.95188501e-02, -3.66509221e-02,\n",
       "       -8.81603267e-03,  7.25093931e-02, -1.08565971e-01, -3.32829989e-02,\n",
       "        4.67546880e-02, -7.60754272e-02, -2.98407837e-03, -7.92452320e-03,\n",
       "       -4.29905392e-02, -8.91508907e-03,  8.67735296e-02, -1.14905588e-01,\n",
       "       -2.73396056e-02, -1.18867852e-01,  7.08254287e-03, -1.03018805e-01,\n",
       "       -2.03065909e-02, -5.78861684e-04,  3.62546965e-02, -6.14150576e-02,\n",
       "        7.21131638e-02, -1.45018786e-01, -1.32339537e-01,  1.31547093e-01,\n",
       "       -4.12075222e-02,  2.41697971e-02, -5.50754406e-02, -5.82452491e-02,\n",
       "       -5.05188368e-02, -3.74433734e-02,  1.01532955e-02,  5.94339259e-02,\n",
       "        4.99244966e-02,  1.10943327e-02, -1.04603708e-01,  4.59622368e-02,\n",
       "        3.11037544e-02, -1.49575379e-02,  8.43961760e-02,  3.20943221e-02,\n",
       "        2.02075355e-02,  6.53773174e-02,  8.61791894e-03,  3.40754502e-02,\n",
       "        3.68985627e-03, -7.84527808e-02,  6.58726040e-03, -1.19858421e-02,\n",
       "        2.85282843e-02, -2.14952696e-02,  9.35093760e-02, -3.54622416e-02,\n",
       "       -1.10943332e-01, -4.71509136e-02, -6.65659979e-02, -2.67452672e-02,\n",
       "        6.63678860e-03, -2.48879567e-03,  1.29764071e-02,  3.96226160e-02,\n",
       "        5.62641174e-02,  5.78490235e-02, -3.54622416e-02, -1.08565971e-01,\n",
       "        4.69528027e-02, -1.05495220e-02,  7.82546680e-03,  1.02226354e-01,\n",
       "        1.86226312e-02,  5.66603430e-02, -1.00245222e-01, -3.74433734e-02,\n",
       "        3.88301648e-02, -2.27830056e-02,  1.47594251e-02,  2.69433800e-02,\n",
       "       -1.89197995e-02,  2.42688525e-02,  4.12075222e-02, -1.90188568e-02,\n",
       "        3.68490331e-02,  8.47924054e-02, -3.16980928e-02, -1.02226354e-01,\n",
       "        1.24415018e-01, -1.27584830e-01, -2.46650800e-02, -1.25999928e-01,\n",
       "       -4.75471430e-02, -2.05047056e-02, -5.54716662e-02,  1.02226354e-01,\n",
       "       -7.76603296e-02, -1.98113092e-04, -3.78395990e-02,  7.96414614e-02,\n",
       "        6.14150576e-02,  1.21245213e-01, -4.37829942e-02, -9.70754120e-03,\n",
       "       -5.98301515e-02, -2.71414928e-02,  4.25943136e-02, -2.32782885e-02,\n",
       "       -4.29905392e-02, -5.07664774e-03,  3.68490331e-02,  4.87358198e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod['school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_mod['school'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- and the word vector for *student* looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01336551,  0.00730235,  0.08037006,  0.05629445,  0.06479172,\n",
       "        0.01141821,  0.11400511, -0.01221483,  0.0559404 , -0.1359564 ,\n",
       "        0.01849928, -0.04779717, -0.04337151, -0.05027555, -0.0073466 ,\n",
       "        0.00477972, -0.02442967, -0.02389859, -0.00907261, -0.05098365,\n",
       "        0.00783343,  0.06302146, -0.00064449, -0.03328099, -0.03540531,\n",
       "       -0.17844278, -0.04956744, -0.002069  ,  0.06160524,  0.0366445 ,\n",
       "        0.03416613, -0.03983098, -0.03080262,  0.01964995, -0.02372156,\n",
       "       -0.00610742,  0.04071611,  0.0506296 , -0.03009452,  0.08072411,\n",
       "       -0.08638897,  0.0426634 , -0.01672901,  0.03115668,  0.0619593 ,\n",
       "       -0.02496075,  0.01451618, -0.03752963,  0.05771066,  0.0179682 ,\n",
       "       -0.03947692, -0.05452418,  0.0054657 , -0.01964995,  0.0906376 ,\n",
       "       -0.03452018, -0.04284043,  0.04036206,  0.07435116, -0.03629045,\n",
       "        0.00610742, -0.03452018, -0.00111748, -0.00489036,  0.03363505,\n",
       "       -0.0306256 ,  0.03080262, -0.02009252,  0.05735661,  0.03823774,\n",
       "       -0.08391059,  0.02991749,  0.00533293, -0.07718358, -0.06479172,\n",
       "        0.1586158 ,  0.05275392, -0.00624019,  0.03115668, -0.00068874,\n",
       "        0.00982497, -0.04921339, -0.00654998,  0.07470521, -0.06160524,\n",
       "        0.00986923, -0.04567285,  0.11117268, -0.01106416,  0.09028355,\n",
       "       -0.04089314, -0.13454019, -0.0559404 , -0.06018903, -0.04301745,\n",
       "        0.02673101,  0.06514578,  0.09559435, -0.0089841 , -0.073289  ,\n",
       "        0.01301145, -0.03823774, -0.03274991, -0.13949694, -0.03469721,\n",
       "       -0.02044657, -0.05239986, -0.06018903,  0.08674302, -0.08497275,\n",
       "       -0.13808072, -0.05912687,  0.03611342,  0.00385033,  0.02991749,\n",
       "       -0.04071611, -0.02903236, -0.01079862,  0.05417013,  0.06337551,\n",
       "        0.02478372,  0.02708506, -0.01787968, -0.05098365,  0.0007247 ,\n",
       "        0.02903236,  0.03770666,  0.00924964,  0.0279702 ,  0.09205382,\n",
       "       -0.01885333, -0.10834026, -0.04744312, -0.03646747, -0.07541332,\n",
       "       -0.04160124,  0.04779717,  0.01761414, -0.12533481,  0.01363105,\n",
       "       -0.12958345, -0.07647548, -0.03310397,  0.02407561, -0.00893984,\n",
       "        0.08568086, -0.0179682 ,  0.10904837,  0.05417013,  0.01646347,\n",
       "        0.14870232,  0.01110842, -0.02071211,  0.01637496, -0.01318848,\n",
       "       -0.07576737,  0.03876882, -0.07116468,  0.05558634,  0.01513577,\n",
       "       -0.10550784,  0.00508951,  0.04372556,  0.08497275, -0.01708306,\n",
       "        0.06727009,  0.06443767, -0.05841877,  0.04708907, -0.01141821,\n",
       "       -0.01708306, -0.01088713,  0.02584588, -0.05771066,  0.0366445 ,\n",
       "        0.01699455,  0.06372956, -0.00042044, -0.01203781,  0.05523229,\n",
       "        0.04390259, -0.05239986,  0.05523229, -0.05983498,  0.0619593 ,\n",
       "        0.02088914,  0.02195129,  0.00564272,  0.00955943, -0.04638096,\n",
       "        0.05275392, -0.01283443, -0.02478372,  0.16074012, -0.15932392,\n",
       "       -0.03292694,  0.06089714, -0.07505926,  0.01128544, -0.01363105,\n",
       "       -0.0619593 ,  0.07718358,  0.03575937, -0.08072411, -0.0846187 ,\n",
       "       -0.12958345, -0.01128544, -0.06125119, -0.03292694, -0.05487823,\n",
       "        0.07789169, -0.04390259,  0.08886734, -0.1352483 , -0.09488624,\n",
       "        0.04372556, -0.03505126,  0.00955943, -0.02832425, -0.01947292,\n",
       "       -0.00482397, -0.10338352,  0.01726009, -0.01602091,  0.02885533,\n",
       "        0.08638897,  0.00840876, -0.02159724,  0.06727009, -0.01504726,\n",
       "        0.07647548,  0.04815122, -0.00242305,  0.06018903, -0.02903236,\n",
       "       -0.04390259,  0.01079862, -0.02460669, -0.00241199,  0.11046458,\n",
       "        0.14162125, -0.05523229, -0.02920938,  0.01469321, -0.04815122,\n",
       "       -0.02637696, -0.11683753,  0.00588613, -0.06833225,  0.06408362,\n",
       "       -0.01557834,  0.00435928,  0.01858779,  0.06018903, -0.03983098,\n",
       "       -0.07789169,  0.01593239,  0.04407962, -0.01911887,  0.05912687,\n",
       "        0.0226594 ,  0.00832025, -0.06762415, -0.11400511, -0.00433715,\n",
       "        0.00840876, -0.0066385 ,  0.06939442, -0.07435116, -0.00272178,\n",
       "        0.03593639, -0.03416613,  0.02257089,  0.08001601, -0.05700256,\n",
       "       -0.0619593 ,  0.00615167, -0.10621594, -0.02655398, -0.06833225,\n",
       "        0.00294307, -0.01787968, -0.04089314,  0.06974847, -0.04779717,\n",
       "        0.03788368, -0.06549983,  0.04301745,  0.1246267 ,  0.08851328,\n",
       "       -0.05487823, -0.04602691, -0.073289  , -0.03328099,  0.02097765,\n",
       "        0.06231335,  0.01389659,  0.04160124,  0.03558234,  0.0313337 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod['student']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the similarity between two word vectors is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60556275"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod.similarity('school', 'student')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for measuring similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "    <td><img src=\"https://nickgrattan.files.wordpress.com/2014/06/screenhunter_76-jun-10-08-36.jpg\" width=\"400\"></td>\n",
    "    <td><img src=\"https://nickgrattan.files.wordpress.com/2014/06/screenhunter_77-jun-10-08-36.jpg\" width=\"400\"></td>\n",
    "    <td><img src=\"https://nickgrattan.files.wordpress.com/2014/06/screenhunter_77-jun-10-08-37.jpg\" width=\"400\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Euclidean distance\n",
    "    - The most common use of distance\n",
    "    - $ \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.615773105863909"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (images from https://nickgrattan.wordpress.com/2014/06/10/euclidean-manhattan-and-cosine-distance-measures-in-c/)\n",
    "np.sqrt(np.power((12-5), 2) + np.power((14-11), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manhattan distance\n",
    "    - Distance = the sum of differences in the grid\n",
    "    - $|x_1 - x_2| + |y_1 - y_2|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(12-5) + np.abs(14-11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine similarity \n",
    "    - Often used to measure similarity between vectors\n",
    "    - $cos(\\theta) = \\frac{\\sum_{i=1}^{n} A_i B_i }{\\sqrt{\\sum_{i=1}^{n} A_i^2 } \\sqrt{\\sum_{i=1}^{n} B_i^2 }}$ \n",
    "    - https://en.wikipedia.org/wiki/Cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9605011450474118"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([12, 14])\n",
    "b = np.array([5, 11])\n",
    "a.dot(b) / (np.sqrt(np.sum(np.power(a, 2))) * np.sqrt(np.sum(np.power(b, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (image from http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine simiarity can go from -1 to 1\n",
    "- But usually, we deal with 0 to 1 scores for comparing words in ```Word2Vec```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q7. What's the cosine similarity between *school* and *tiger*? </font>\n",
    "- How would you interprete the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words 'school' and 'tiger' are not very related to each other, that their correlation is only 0.09, that's pretty close to 0 (meaning no relations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08660267"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod.similarity('school', 'tiger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q8. Try some other words. Any other interesting findings? </font>\n",
    "- Give 3 more examples.\n",
    "- How would you interprete the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8092215"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod.similarity('black', 'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5121041"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod.similarity('lion', 'tiger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22693524"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod.similarity('Chris', 'Christian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see the 2 words black and white have strong correlations, I think it's because they are both colors. Words lion and tiger have medium level of correlations, maybe because they are both animals, or mammals, but different species. Chris and Christian don't have strong correlations, which surprised me, because I thought they are same names, just the previous one is the short version. Clearly that's not the way how machine think through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy from word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/linear-relationships.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we approximate the relationship between words by doing - and + operations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $woman - man + king \\approx ?$\n",
    "- How this works?\n",
    "    - $woman:man \\approx x:king $\n",
    "    - $\\rightarrow woman - man \\approx x - king $\n",
    "    - $\\rightarrow woman - man + king \\approx x$\n",
    "    - List top-10 words ($x$) that can solve the equation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431607246399),\n",
       " ('prince', 0.5377322435379028),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454),\n",
       " ('throne', 0.5005807280540466),\n",
       " ('royal', 0.4938204884529114)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Spain - Germany + Berlin \\approx ?$\n",
    "    - $\\rightarrow Spain - Germany \\approx x -  Berlin $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Madrid', 0.717348039150238),\n",
       " ('Barcelona', 0.5742595791816711),\n",
       " ('Malaga', 0.5547429323196411),\n",
       " ('Bilbao', 0.5404423475265503),\n",
       " ('Lisbon', 0.5366039276123047),\n",
       " ('Seville', 0.5312415361404419),\n",
       " ('Paris', 0.5251091718673706),\n",
       " ('Catalan', 0.5213973522186279),\n",
       " ('Rome', 0.5174577832221985),\n",
       " ('Bogota', 0.5146557688713074)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod.most_similar(positive=['Spain', 'Berlin'], negative=['Germany'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'> Q9. Any other interesting examples? </font>\n",
    "- Give 3 more examples.\n",
    "- How would you interprete the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Montreal', 0.4874575138092041),\n",
       " ('Hamburg', 0.4286065697669983),\n",
       " ('Germany', 0.4281412363052368),\n",
       " ('Ottawa', 0.42371922731399536),\n",
       " ('Munich', 0.41592979431152344),\n",
       " ('Canadian', 0.4144042432308197),\n",
       " ('Toronto', 0.4142145812511444),\n",
       " ('Dessau', 0.412375271320343),\n",
       " ('Ontario', 0.4114670753479004),\n",
       " ('Kitchener', 0.4088645279407501)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod.most_similar(positive=['Canada', 'Berlin'], negative=['China'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Canadian', 0.661740779876709),\n",
       " ('Toronto', 0.6385330557823181),\n",
       " ('Montreal', 0.635225772857666),\n",
       " ('Ottawa', 0.6350735425949097),\n",
       " ('Vancouver', 0.6248366832733154),\n",
       " ('Calgary', 0.617638349533081),\n",
       " ('Winnipeg', 0.6019836664199829),\n",
       " ('Saskatoon', 0.5973019599914551),\n",
       " ('Ontario', 0.5954508781433105),\n",
       " ('Canadians', 0.5915849208831787)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod.most_similar(positive=['Canada', 'Beijing'], negative=['China'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('puppy', 0.769972562789917),\n",
       " ('pup', 0.6861710548400879),\n",
       " ('dogs', 0.6770987510681152),\n",
       " ('Rottweiler', 0.66466224193573),\n",
       " ('Pomeranian', 0.6553813815116882),\n",
       " ('puppies', 0.6450917720794678),\n",
       " ('pooch', 0.6447545289993286),\n",
       " ('Sheltie', 0.624342679977417),\n",
       " ('schnauzer', 0.6071555614471436),\n",
       " ('chihuahua', 0.6056430339813232)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mod.most_similar(positive=['dog', 'kitten'], negative=['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 examples are interesting. First one, I use Berlin and China, kind of not related two words, for Canada, and the most related item is Montreal, a city of Canada. I was expecting some city name outside Canada. Then for the second one, I use Beijing and China, to expect the output will be the capital of Canada, but interestingly, the most related word is Canadian, and Toranto is the second most related word. Lastly I used the example from class, and the result is as expected: dog-puupy & cat-kitten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing interpretable semantic scales "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far, we saw that word vectors effectively carries (although not perfect) the semantic information.\n",
    "- Can we design something more interpretable results from using the semantic space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's re-try with real datapoints in [here](https://projector.tensorflow.org): *politics* words in a *bad-good* PCA space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    " \n",
    "def cosine_similarity(x, y):\n",
    "    return(1 - spatial.distance.cosine(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can we regenerate this results with our embedding model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot words in the 2D space\n",
    "- Using Bad & Good axes\n",
    "- Calculate cosine similarity between an evaluating word (violence, discussion, and issues) with each scale's end (bad, and good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_words_sim_2d = pd.DataFrame([[cosine_similarity(w2v_mod['violence'], w2v_mod['good']), cosine_similarity(w2v_mod['violence'], w2v_mod['bad'])],\n",
    "                                 [cosine_similarity(w2v_mod['discussion'], w2v_mod['good']), cosine_similarity(w2v_mod['discussion'], w2v_mod['bad'])],\n",
    "                                 [cosine_similarity(w2v_mod['issues'], w2v_mod['good']), cosine_similarity(w2v_mod['issues'], w2v_mod['bad'])]],\n",
    "                                index=['violence', 'discussion', 'issues'], columns=['good', 'bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>violence</td>\n",
       "      <td>-0.016942</td>\n",
       "      <td>0.091223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>discussion</td>\n",
       "      <td>0.045935</td>\n",
       "      <td>-0.007224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>issues</td>\n",
       "      <td>0.139897</td>\n",
       "      <td>0.118816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                good       bad\n",
       "violence   -0.016942  0.091223\n",
       "discussion  0.045935 -0.007224\n",
       "issues      0.139897  0.118816"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_words_sim_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14d319f90>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x='good', y='bad', data=pol_words_sim_2d, hue=pol_words_sim_2d.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- violence: less good, more bad\n",
    "- discussion: less bad, more good\n",
    "- issues: both bad and good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do this in an 1D scale?\n",
    "(bad) --------------------?---- (good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, let's create the vector for *bad-good* scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_bad_good = w2v_mod['good'] - w2v_mod['bad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the cosine similarity score of the word *violence* in the *bad-good* scale \n",
    "    - $sim(V(violence), V(bad) - V(good))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14428630471229553"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(w2v_mod['violence'], scale_bad_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $sim(V(discussion), V(bad) - V(good))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0709114670753479"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(w2v_mod['discussion'], w2v_mod['good'] - w2v_mod['bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $sim(V(issues), V(bad) - V(good))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028121288865804672"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(w2v_mod['issues'], w2v_mod['good'] - w2v_mod['bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, as displayed in Embedding Projector, words *violence*, *discussion*, and *issues* are located in the *bad-good* scale as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_words_sim = pd.DataFrame([cosine_similarity(w2v_mod['violence'], w2v_mod['good'] - w2v_mod['bad']),\n",
    "                              cosine_similarity(w2v_mod['discussion'], w2v_mod['good'] - w2v_mod['bad']),\n",
    "                              cosine_similarity(w2v_mod['issues'], w2v_mod['good'] - w2v_mod['bad'])],\n",
    "                             index=['violence', 'discussion', 'issues'], columns=['cos_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>violence</td>\n",
       "      <td>-0.144286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>discussion</td>\n",
       "      <td>0.070911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>issues</td>\n",
       "      <td>0.028121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             cos_sim\n",
       "violence   -0.144286\n",
       "discussion  0.070911\n",
       "issues      0.028121"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_words_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.barplot(x=pol_words_sim.index, y=pol_words_sim.cos_sim)\n",
    "ax.set(ylabel=\"bad_good scale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Violence* is more close to the *bad* end of the scale, while *discussion* is located towards the *good* end of the scale. *Issues* is located between those two words in the *bad-good* scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='magenta'>Q Stretch:  Select a different scale and a set of words. How are words  represented in your suggested semantic scale? </font>\n",
    "- Why did you selected the particular scale and words? what's your interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(type in your response here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- more to read about this method:     http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
